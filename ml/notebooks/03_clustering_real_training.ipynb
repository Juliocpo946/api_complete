{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering con Datos Reales\n",
    "## Entrenamiento para uso futuro (actualmente sin datos reales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "from database_connection import execute_query\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verificar Disponibilidad de Datos Reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_count = \"\"\"\n",
    "SELECT \n",
    "    COUNT(DISTINCT s.user_id) as total_users,\n",
    "    COUNT(DISTINCT s.session_id) as total_sessions,\n",
    "    COUNT(DISTINCT ua.activity_uuid) as total_activities,\n",
    "    MIN(s.created_at) as first_session,\n",
    "    MAX(s.created_at) as last_session\n",
    "FROM session_service_test.sessions s\n",
    "LEFT JOIN session_service_test.user_activities ua ON s.session_id = ua.session_id\n",
    "WHERE s.user_id NOT BETWEEN 1001 AND 1100\n",
    "\"\"\"\n",
    "\n",
    "real_data_stats = execute_query(query_count)\n",
    "\n",
    "print(\"ESTADÍSTICAS DE DATOS REALES:\")\n",
    "print(\"=\"*60)\n",
    "if real_data_stats:\n",
    "    stats = real_data_stats[0]\n",
    "    print(f\"Total usuarios reales: {stats['total_users']}\")\n",
    "    print(f\"Total sesiones: {stats['total_sessions']}\")\n",
    "    print(f\"Total actividades: {stats['total_activities']}\")\n",
    "    print(f\"Primera sesión: {stats['first_session']}\")\n",
    "    print(f\"Última sesión: {stats['last_session']}\")\n",
    "    \n",
    "    MIN_USERS = 50\n",
    "    MIN_ACTIVITIES = 5\n",
    "    \n",
    "    if stats['total_users'] >= MIN_USERS:\n",
    "        print(f\"\\n✓ Suficientes usuarios para entrenar ({stats['total_users']} >= {MIN_USERS})\")\n",
    "        CAN_TRAIN = True\n",
    "    else:\n",
    "        print(f\"\\n✗ Insuficientes usuarios ({stats['total_users']} < {MIN_USERS})\")\n",
    "        print(f\"  Se necesitan al menos {MIN_USERS - stats['total_users']} usuarios más\")\n",
    "        CAN_TRAIN = False\n",
    "else:\n",
    "    print(\"No hay datos reales disponibles todavía\")\n",
    "    CAN_TRAIN = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar Datos Reales (si están disponibles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CAN_TRAIN:\n",
    "    query_real = \"\"\"\n",
    "    WITH user_activity_stats AS (\n",
    "        SELECT \n",
    "            s.user_id,\n",
    "            COUNT(DISTINCT ua.activity_uuid) as total_activities,\n",
    "            SUM(CASE WHEN ua.status = 'completed' THEN 1 ELSE 0 END) as completed_activities,\n",
    "            SUM(CASE WHEN ua.status = 'abandoned' THEN 1 ELSE 0 END) as abandoned_activities,\n",
    "            AVG(ua.pause_count) as avg_pause_count,\n",
    "            AVG(TIMESTAMPDIFF(MINUTE, ua.started_at, ua.completed_at)) as avg_activity_duration_minutes\n",
    "        FROM session_service_test.sessions s\n",
    "        JOIN session_service_test.user_activities ua ON s.session_id = ua.session_id\n",
    "        WHERE ua.started_at IS NOT NULL\n",
    "        AND s.user_id NOT BETWEEN 1001 AND 1100\n",
    "        GROUP BY s.user_id\n",
    "    ),\n",
    "    emotion_stats AS (\n",
    "        SELECT \n",
    "            s.user_id,\n",
    "            AVG(CASE WHEN ms.predominant_emotion = 'Angry' THEN ms.emotion_confidence_avg ELSE 0 END) as avg_frustration,\n",
    "            AVG(ms.looking_screen_percentage) as avg_visual_attention,\n",
    "            AVG(ms.ear_avg) as avg_ear,\n",
    "            AVG(CASE WHEN ms.engagement_level = 'high' THEN 3 \n",
    "                     WHEN ms.engagement_level = 'medium' THEN 2 \n",
    "                     ELSE 1 END) as avg_engagement_score\n",
    "        FROM session_service_test.sessions s\n",
    "        JOIN session_service_test.user_activities ua ON s.session_id = ua.session_id\n",
    "        JOIN monitoring_service_test.minute_summaries ms ON ua.activity_uuid = ms.activity_uuid\n",
    "        WHERE s.user_id NOT BETWEEN 1001 AND 1100\n",
    "        GROUP BY s.user_id\n",
    "    ),\n",
    "    distraction_stats AS (\n",
    "        SELECT \n",
    "            s.user_id,\n",
    "            SUM(ms.distraction_count) as total_distraction_events,\n",
    "            SUM(ms.drowsiness_count) as total_drowsiness_events,\n",
    "            SUM(TIMESTAMPDIFF(HOUR, ua.started_at, COALESCE(ua.completed_at, NOW()))) as total_hours\n",
    "        FROM session_service_test.sessions s\n",
    "        JOIN session_service_test.user_activities ua ON s.session_id = ua.session_id\n",
    "        JOIN monitoring_service_test.minute_summaries ms ON ua.activity_uuid = ms.activity_uuid\n",
    "        WHERE s.user_id NOT BETWEEN 1001 AND 1100\n",
    "        GROUP BY s.user_id\n",
    "    ),\n",
    "    intervention_stats AS (\n",
    "        SELECT \n",
    "            s.user_id,\n",
    "            COUNT(i.packet_id) as total_interventions,\n",
    "            SUM(CASE WHEN i.intervention_type = 'video_instruction' THEN 1 ELSE 0 END) as video_interventions,\n",
    "            SUM(CASE WHEN i.intervention_type = 'text_instruction' THEN 1 ELSE 0 END) as text_interventions,\n",
    "            SUM(CASE WHEN i.intervention_type = 'vibration_only' THEN 1 ELSE 0 END) as vibration_interventions\n",
    "        FROM session_service_test.sessions s\n",
    "        JOIN session_service_test.user_activities ua ON s.session_id = ua.session_id\n",
    "        JOIN monitoring_service_test.interventions i ON ua.activity_uuid = i.activity_uuid\n",
    "        WHERE s.user_id NOT BETWEEN 1001 AND 1100\n",
    "        GROUP BY s.user_id\n",
    "    ),\n",
    "    activity_type_performance AS (\n",
    "        SELECT \n",
    "            s.user_id,\n",
    "            SUM(CASE WHEN am.activity_type IN ('tracing', 'memory_game') AND ua.status = 'completed' THEN 1 ELSE 0 END) as easy_completed,\n",
    "            SUM(CASE WHEN am.activity_type IN ('tracing', 'memory_game') THEN 1 ELSE 0 END) as easy_total,\n",
    "            SUM(CASE WHEN am.activity_type IN ('fill_in_blank', 'reading_comprehension') AND ua.status = 'completed' THEN 1 ELSE 0 END) as hard_completed,\n",
    "            SUM(CASE WHEN am.activity_type IN ('fill_in_blank', 'reading_comprehension') THEN 1 ELSE 0 END) as hard_total\n",
    "        FROM session_service_test.sessions s\n",
    "        JOIN session_service_test.user_activities ua ON s.session_id = ua.session_id\n",
    "        JOIN session_service_test.activity_masters am ON ua.external_activity_id = am.external_activity_id\n",
    "        WHERE s.user_id NOT BETWEEN 1001 AND 1100\n",
    "        GROUP BY s.user_id\n",
    "    ),\n",
    "    visual_fatigue_stats AS (\n",
    "        SELECT \n",
    "            s.user_id,\n",
    "            SUM(CASE WHEN ms.ear_avg < 0.25 THEN 1 ELSE 0 END) as low_ear_count,\n",
    "            COUNT(*) as total_minutes\n",
    "        FROM session_service_test.sessions s\n",
    "        JOIN session_service_test.user_activities ua ON s.session_id = ua.session_id\n",
    "        JOIN monitoring_service_test.minute_summaries ms ON ua.activity_uuid = ms.activity_uuid\n",
    "        WHERE s.user_id NOT BETWEEN 1001 AND 1100\n",
    "        GROUP BY s.user_id\n",
    "    )\n",
    "    SELECT \n",
    "        uas.user_id,\n",
    "        COALESCE(uas.completed_activities / NULLIF(uas.total_activities, 0), 0) as completion_rate,\n",
    "        COALESCE(uas.abandoned_activities / NULLIF(uas.total_activities, 0), 0) as abandonment_rate,\n",
    "        COALESCE(es.avg_frustration, 0) as avg_frustration,\n",
    "        COALESCE(es.avg_visual_attention, 0) as avg_visual_attention,\n",
    "        COALESCE(es.avg_engagement_score, 1) as avg_engagement_score,\n",
    "        COALESCE(ds.total_distraction_events / NULLIF(ds.total_hours, 0), 0) as distraction_events_per_hour,\n",
    "        COALESCE(ds.total_drowsiness_events / NULLIF(ds.total_hours, 0), 0) as drowsiness_events_per_hour,\n",
    "        COALESCE(vfs.low_ear_count / NULLIF(vfs.total_minutes, 0), 0) as avg_visual_fatigue,\n",
    "        COALESCE(uas.avg_pause_count, 0) as avg_pause_count,\n",
    "        COALESCE(uas.avg_activity_duration_minutes, 0) as avg_activity_duration_minutes,\n",
    "        COALESCE(is_total.total_interventions / NULLIF(uas.total_activities, 0), 0) as intervention_count_per_activity,\n",
    "        COALESCE(is_total.video_interventions / NULLIF(is_total.total_interventions, 0), 0) as response_to_video,\n",
    "        COALESCE(is_total.text_interventions / NULLIF(is_total.total_interventions, 0), 0) as response_to_text,\n",
    "        COALESCE(is_total.vibration_interventions / NULLIF(is_total.total_interventions, 0), 0) as response_to_vibration,\n",
    "        COALESCE(atp.easy_completed / NULLIF(atp.easy_total, 0), 0) as preference_easy_activities\n",
    "    FROM user_activity_stats uas\n",
    "    LEFT JOIN emotion_stats es ON uas.user_id = es.user_id\n",
    "    LEFT JOIN distraction_stats ds ON uas.user_id = ds.user_id\n",
    "    LEFT JOIN intervention_stats is_total ON uas.user_id = is_total.user_id\n",
    "    LEFT JOIN activity_type_performance atp ON uas.user_id = atp.user_id\n",
    "    LEFT JOIN visual_fatigue_stats vfs ON uas.user_id = vfs.user_id\n",
    "    WHERE uas.total_activities >= 5\n",
    "    ORDER BY uas.user_id\n",
    "    \"\"\"\n",
    "    \n",
    "    real_data = execute_query(query_real)\n",
    "    df_real = pd.DataFrame(real_data)\n",
    "    \n",
    "    print(f\"Datos reales cargados: {df_real.shape[0]} usuarios\")\n",
    "    print(f\"\\nPrimeros registros:\")\n",
    "    display(df_real.head())\n",
    "else:\n",
    "    print(\"No hay suficientes datos reales para entrenar\")\n",
    "    print(\"Este notebook quedará como template para cuando estén disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparación: Datos Fake vs Datos Reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CAN_TRAIN:\n",
    "    df_fake = pd.read_csv('../output/features_for_clustering.csv')\n",
    "    \n",
    "    feature_cols = [col for col in df_fake.columns if col != 'user_id']\n",
    "    \n",
    "    comparison_stats = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Fake_Mean': df_fake[feature_cols].mean().values,\n",
    "        'Real_Mean': df_real[feature_cols].mean().values,\n",
    "        'Fake_Std': df_fake[feature_cols].std().values,\n",
    "        'Real_Std': df_real[feature_cols].std().values\n",
    "    })\n",
    "    \n",
    "    comparison_stats['Diff_%'] = ((comparison_stats['Real_Mean'] - comparison_stats['Fake_Mean']) / \n",
    "                                   comparison_stats['Fake_Mean'] * 100).round(2)\n",
    "    \n",
    "    print(\"COMPARACIÓN FAKE vs REAL:\")\n",
    "    print(\"=\"*80)\n",
    "    display(comparison_stats)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, feature in enumerate(feature_cols[:9]):\n",
    "        axes[idx].hist(df_fake[feature], bins=20, alpha=0.5, label='Fake', color='blue')\n",
    "        axes[idx].hist(df_real[feature], bins=20, alpha=0.5, label='Real', color='red')\n",
    "        axes[idx].set_title(feature, fontsize=10)\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../output/fake_vs_real_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Comparación no disponible - esperando datos reales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocesamiento de Datos Reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CAN_TRAIN:\n",
    "    feature_cols = [col for col in df_real.columns if col != 'user_id']\n",
    "    X_real = df_real[feature_cols].values\n",
    "    user_ids_real = df_real['user_id'].values\n",
    "    \n",
    "    scaler_real = StandardScaler()\n",
    "    X_real_scaled = scaler_real.fit_transform(X_real)\n",
    "    \n",
    "    print(f\"Datos normalizados: {X_real_scaled.shape}\")\n",
    "    print(f\"\\nEstadísticas después de normalización:\")\n",
    "    print(f\"  Media: {X_real_scaled.mean(axis=0).round(3)}\")\n",
    "    print(f\"  Desviación estándar: {X_real_scaled.std(axis=0).round(3)}\")\n",
    "else:\n",
    "    print(\"Esperando datos reales para preprocesar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Determinar K Óptimo con Datos Reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CAN_TRAIN:\n",
    "    inertias_real = []\n",
    "    silhouette_scores_real = []\n",
    "    calinski_scores_real = []\n",
    "    davies_bouldin_scores_real = []\n",
    "    K_range = range(2, 9)\n",
    "    \n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
    "        labels = kmeans.fit_predict(X_real_scaled)\n",
    "        \n",
    "        inertias_real.append(kmeans.inertia_)\n",
    "        silhouette_scores_real.append(silhouette_score(X_real_scaled, labels))\n",
    "        calinski_scores_real.append(calinski_harabasz_score(X_real_scaled, labels))\n",
    "        davies_bouldin_scores_real.append(davies_bouldin_score(X_real_scaled, labels))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    axes[0, 0].plot(K_range, inertias_real, 'bo-', linewidth=2, markersize=8)\n",
    "    axes[0, 0].set_xlabel('K')\n",
    "    axes[0, 0].set_ylabel('Inercia')\n",
    "    axes[0, 0].set_title('Método del Codo (Datos Reales)')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].plot(K_range, silhouette_scores_real, 'ro-', linewidth=2, markersize=8)\n",
    "    axes[0, 1].set_xlabel('K')\n",
    "    axes[0, 1].set_ylabel('Silhouette Score')\n",
    "    axes[0, 1].set_title('Silhouette Score (Datos Reales)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 0].plot(K_range, calinski_scores_real, 'go-', linewidth=2, markersize=8)\n",
    "    axes[1, 0].set_xlabel('K')\n",
    "    axes[1, 0].set_ylabel('Calinski-Harabasz Score')\n",
    "    axes[1, 0].set_title('Calinski-Harabasz (mayor es mejor)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].plot(K_range, davies_bouldin_scores_real, 'mo-', linewidth=2, markersize=8)\n",
    "    axes[1, 1].set_xlabel('K')\n",
    "    axes[1, 1].set_ylabel('Davies-Bouldin Score')\n",
    "    axes[1, 1].set_title('Davies-Bouldin (menor es mejor)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../output/real_data_elbow_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    metrics_real_df = pd.DataFrame({\n",
    "        'K': list(K_range),\n",
    "        'Inercia': inertias_real,\n",
    "        'Silhouette': silhouette_scores_real,\n",
    "        'Calinski': calinski_scores_real,\n",
    "        'Davies-Bouldin': davies_bouldin_scores_real\n",
    "    })\n",
    "    \n",
    "    print(\"\\nMétricas por K (Datos Reales):\")\n",
    "    display(metrics_real_df)\n",
    "    \n",
    "    optimal_k_real = silhouette_scores_real.index(max(silhouette_scores_real)) + 2\n",
    "    print(f\"\\nK óptimo sugerido (Silhouette): {optimal_k_real}\")\n",
    "else:\n",
    "    print(\"Esperando datos reales para determinar K óptimo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entrenar Modelo Final con Datos Reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CAN_TRAIN:\n",
    "    optimal_k_real = 4\n",
    "    \n",
    "    kmeans_real = KMeans(n_clusters=optimal_k_real, random_state=42, n_init=30)\n",
    "    cluster_labels_real = kmeans_real.fit_predict(X_real_scaled)\n",
    "    \n",
    "    df_real['cluster'] = cluster_labels_real\n",
    "    \n",
    "    silhouette_real = silhouette_score(X_real_scaled, cluster_labels_real)\n",
    "    davies_bouldin_real = davies_bouldin_score(X_real_scaled, cluster_labels_real)\n",
    "    calinski_real = calinski_harabasz_score(X_real_scaled, cluster_labels_real)\n",
    "    \n",
    "    print(f\"MODELO ENTRENADO CON DATOS REALES (K={optimal_k_real})\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nMétricas:\")\n",
    "    print(f\"  - Silhouette Score: {silhouette_real:.3f}\")\n",
    "    print(f\"  - Davies-Bouldin: {davies_bouldin_real:.3f}\")\n",
    "    print(f\"  - Calinski-Harabasz: {calinski_real:.2f}\")\n",
    "    print(f\"  - Inercia: {kmeans_real.inertia_:.2f}\")\n",
    "    \n",
    "    print(f\"\\nDistribución de clusters:\")\n",
    "    cluster_counts_real = pd.Series(cluster_labels_real).value_counts().sort_index()\n",
    "    for cluster_id, count in cluster_counts_real.items():\n",
    "        percentage = (count / len(cluster_labels_real)) * 100\n",
    "        print(f\"  Cluster {cluster_id}: {count} usuarios ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"Esperando datos reales para entrenar modelo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interpretación de Clusters Reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CAN_TRAIN:\n",
    "    cluster_stats_real = df_real.groupby('cluster')[feature_cols].mean()\n",
    "    \n",
    "    print(\"CARACTERÍSTICAS DE CLUSTERS (DATOS REALES):\")\n",
    "    print(\"=\"*80)\n",
    "    display(cluster_stats_real.round(3))\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(cluster_stats_real.T, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "                center=cluster_stats_real.values.mean())\n",
    "    plt.title('Centroides de Clusters Reales', fontsize=14, pad=20)\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../output/real_clusters_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    cluster_labels_real_map = {}\n",
    "    for cluster_id in range(optimal_k_real):\n",
    "        stats = cluster_stats_real.loc[cluster_id]\n",
    "        \n",
    "        if stats['completion_rate'] > 0.7 and stats['avg_frustration'] < 0.45:\n",
    "            label = \"Rápido Visual (Real)\"\n",
    "        elif stats['avg_frustration'] > 0.55 and stats['completion_rate'] > 0.55:\n",
    "            label = \"Lector Constante (Real)\"\n",
    "        elif stats['distraction_events_per_hour'] > 6:\n",
    "            label = \"Disperso Visual (Real)\"\n",
    "        else:\n",
    "            label = \"Fatigado Visual (Real)\"\n",
    "        \n",
    "        cluster_labels_real_map[cluster_id] = label\n",
    "        \n",
    "        count = (cluster_labels_real == cluster_id).sum()\n",
    "        percentage = (count / len(cluster_labels_real)) * 100\n",
    "        \n",
    "        print(f\"\\nCluster {cluster_id}: {label}\")\n",
    "        print(f\"  N = {count} ({percentage:.1f}%)\")\n",
    "        print(f\"  Completion: {stats['completion_rate']:.2%}\")\n",
    "        print(f\"  Frustración: {stats['avg_frustration']:.3f}\")\n",
    "        print(f\"  Atención: {stats['avg_visual_attention']:.1f}%\")\n",
    "        print(f\"  Distracción/h: {stats['distraction_events_per_hour']:.2f}\")\n",
    "else:\n",
    "    print(\"Esperando datos reales para interpretación\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualización PCA (Datos Reales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CAN_TRAIN:\n",
    "    pca_real = PCA(n_components=2, random_state=42)\n",
    "    X_pca_real = pca_real.fit_transform(X_real_scaled)\n",
    "    \n",
    "    print(f\"Varianza explicada (Datos Reales):\")\n",
    "    print(f\"  PC1: {pca_real.explained_variance_ratio_[0]:.2%}\")\n",
    "    print(f\"  PC2: {pca_real.explained_variance_ratio_[1]:.2%}\")\n",
    "    print(f\"  Total: {pca_real.explained_variance_ratio_.sum():.2%}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "    \n",
    "    for cluster_id in range(optimal_k_real):\n",
    "        mask = cluster_labels_real == cluster_id\n",
    "        plt.scatter(X_pca_real[mask, 0], X_pca_real[mask, 1],\n",
    "                   c=colors[cluster_id], label=cluster_labels_real_map[cluster_id],\n",
    "                   alpha=0.6, s=100, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    centroids_pca_real = pca_real.transform(kmeans_real.cluster_centers_)\n",
    "    plt.scatter(centroids_pca_real[:, 0], centroids_pca_real[:, 1],\n",
    "               c='black', marker='X', s=300, edgecolors='yellow', linewidth=2,\n",
    "               label='Centroides', zorder=5)\n",
    "    \n",
    "    plt.xlabel(f'PC1 ({pca_real.explained_variance_ratio_[0]:.1%})')\n",
    "    plt.ylabel(f'PC2 ({pca_real.explained_variance_ratio_[1]:.1%})')\n",
    "    plt.title('Clusters con Datos Reales (PCA)', fontsize=14, pad=20)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../output/real_clusters_pca.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Esperando datos reales para visualización\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Guardar Modelo con Datos Reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CAN_TRAIN:\n",
    "    joblib.dump(kmeans_real, '../models/user_type_classifier_REAL.pkl')\n",
    "    joblib.dump(scaler_real, '../models/scaler_REAL.pkl')\n",
    "    \n",
    "    labels_real_export = {str(k): v for k, v in cluster_labels_real_map.items()}\n",
    "    with open('../models/cluster_labels_REAL.json', 'w') as f:\n",
    "        json.dump(labels_real_export, f, indent=2)\n",
    "    \n",
    "    model_metadata_real = {\n",
    "        'n_clusters': optimal_k_real,\n",
    "        'features': feature_cols,\n",
    "        'silhouette_score': float(silhouette_real),\n",
    "        'davies_bouldin_score': float(davies_bouldin_real),\n",
    "        'calinski_harabasz_score': float(calinski_real),\n",
    "        'inertia': float(kmeans_real.inertia_),\n",
    "        'cluster_distribution': {str(k): int(v) for k, v in cluster_counts_real.items()},\n",
    "        'trained_on': datetime.now().isoformat(),\n",
    "        'data_source': 'REAL',\n",
    "        'total_users': len(df_real)\n",
    "    }\n",
    "    \n",
    "    with open('../models/model_metadata_REAL.json', 'w') as f:\n",
    "        json.dump(model_metadata_real, f, indent=2)\n",
    "    \n",
    "    df_real.to_csv('../output/real_users_with_clusters.csv', index=False)\n",
    "    \n",
    "    print(\"MODELO CON DATOS REALES GUARDADO:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"  ✓ models/user_type_classifier_REAL.pkl\")\n",
    "    print(\"  ✓ models/scaler_REAL.pkl\")\n",
    "    print(\"  ✓ models/cluster_labels_REAL.json\")\n",
    "    print(\"  ✓ models/model_metadata_REAL.json\")\n",
    "    print(\"  ✓ output/real_users_with_clusters.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SIGUIENTE PASO:\")\n",
    "    print(\"  1. Comparar rendimiento fake vs real\")\n",
    "    print(\"  2. Si el modelo real es mejor, reemplazar el fake en producción\")\n",
    "    print(\"  3. Actualizar predict_user_type.py para usar modelo REAL\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"\\nMODELO EN ESPERA\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Este notebook está listo para ejecutarse cuando haya suficientes datos reales.\")\n",
    "    print(f\"\\nRequisitos mínimos:\")\n",
    "    print(f\"  - Al menos 50 usuarios reales\")\n",
    "    print(f\"  - Al menos 5 actividades por usuario\")\n",
    "    print(f\"\\nEstado actual: {real_data_stats[0]['total_users'] if real_data_stats else 0} usuarios reales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Pipeline Automatizado de Reentrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automated_retraining_pipeline():\n",
    "    \"\"\"\n",
    "    Pipeline completo para reentrenar el modelo automáticamente\n",
    "    Puede ser llamado por un cron job o scheduler\n",
    "    \"\"\"\n",
    "    print(\"INICIANDO PIPELINE DE REENTRENAMIENTO AUTOMÁTICO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Verificar disponibilidad de datos\n",
    "    query_count = \"\"\"\n",
    "    SELECT COUNT(DISTINCT s.user_id) as total_users\n",
    "    FROM session_service_test.sessions s\n",
    "    JOIN session_service_test.user_activities ua ON s.session_id = ua.session_id\n",
    "    WHERE s.user_id NOT BETWEEN 1001 AND 1100\n",
    "    AND ua.status IN ('completed', 'abandoned')\n",
    "    GROUP BY s.user_id\n",
    "    HAVING COUNT(DISTINCT ua.activity_uuid) >= 5\n",
    "    \"\"\"\n",
    "    \n",
    "    real_data_stats = execute_query(query_count)\n",
    "    n_users = len(real_data_stats) if real_data_stats else 0\n",
    "    \n",
    "    print(f\"Usuarios disponibles: {n_users}\")\n",
    "    \n",
    "    if n_users < 50:\n",
    "        print(f\"Insuficientes datos. Se necesitan al menos 50 usuarios con 5+ actividades.\")\n",
    "        print(f\"Faltan: {50 - n_users} usuarios\")\n",
    "        return False\n",
    "    \n",
    "    # 2. Cargar y preparar datos\n",
    "    print(\"\\nCargando datos reales...\")\n",
    "    # (usar query_real del paso 2)\n",
    "    \n",
    "    # 3. Entrenar modelo\n",
    "    print(\"\\nEntrenando modelo...\")\n",
    "    # (usar código del paso 6)\n",
    "    \n",
    "    # 4. Validar mejora sobre modelo anterior\n",
    "    print(\"\\nValidando mejora...\")\n",
    "    try:\n",
    "        old_metadata = json.load(open('../models/model_metadata_REAL.json'))\n",
    "        old_silhouette = old_metadata['silhouette_score']\n",
    "        \n",
    "        if silhouette_real > old_silhouette:\n",
    "            print(f\"✓ Mejora detectada: {old_silhouette:.3f} → {silhouette_real:.3f}\")\n",
    "            # Guardar nuevo modelo\n",
    "        else:\n",
    "            print(f\"✗ No hay mejora: {old_silhouette:.3f} → {silhouette_real:.3f}\")\n",
    "            print(\"  Manteniendo modelo anterior\")\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"Primer entrenamiento con datos reales\")\n",
    "        # Guardar modelo\n",
    "    \n",
    "    # 5. Guardar modelo\n",
    "    print(\"\\nGuardando modelo...\")\n",
    "    # (usar código del paso 9)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"REENTRENAMIENTO COMPLETADO EXITOSAMENTE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"Pipeline de reentrenamiento automático definido\")\n",
    "print(\"Para ejecutar manualmente: automated_retraining_pipeline()\")\n",
    "print(\"Para automatizar: configurar cron job que ejecute este notebook\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}